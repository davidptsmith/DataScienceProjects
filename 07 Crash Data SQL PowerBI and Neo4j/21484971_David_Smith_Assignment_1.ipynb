{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Crash Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Notebook Logic Flow Control\n",
    "\n",
    "This class is used to control the flow of the notebook. Set elements to tru that you wish to run. \n",
    "\n",
    "Note:  \n",
    "Visualisations and other non-essential functions have been grouped under 'assignment_1_not_needed' as these do not need to be reviewed as a part of assignment 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FunctionsToRun: \n",
    "    ## Imports and Setups \n",
    "    import_data = True\n",
    "    transform = True\n",
    "    subset_data = True\n",
    "\n",
    "    ## Run Relational Database Functions\n",
    "    relational_data = False\n",
    "\n",
    "    ## Run Neo4j Functions\n",
    "    neo4j = True\n",
    "    ## General info & Assignment 1\n",
    "    assignment_1_not_needed = False\n",
    "\n",
    "    ## Base true as place holder\n",
    "    true = True\n",
    "functions_to_run = FunctionsToRun()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "import os\n",
    "import pandas as pd \n",
    "import geopandas\n",
    "import numpy as np\n",
    "\n",
    "## Import Graphics Libraries \n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Setup Colour Scheme \n",
    "sns.set_palette(\"tab10\")\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\",  rc=custom_params)\n",
    "\n",
    "## Allows for multiple outputs from cells without having to repeatedly write display \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Set the size of the plots \n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100 # Note: 200 e.g. is really fine, but slower\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Data Cleaning Pipeline \n",
    "\n",
    "1. Load and review data structure - ensure fields are matching and structure is the same. \n",
    "2. Enrich data\n",
    "3. Check and log missing values \n",
    "4. Check and log duplicate values \n",
    "5. Clean and Enforce Standards \n",
    "6. load data into tables \n",
    "\n",
    "\n",
    "*Note: Unfortunately due to time, the meta data table has not being fully implemented.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.assignment_1_not_needed:\n",
    "    from IPython import display\n",
    "    display.Image(\"./presentation/diagrams/4x/ETL@4x.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.import_data:\n",
    "    crash_gdf = geopandas.read_file(\"./data/Crash_Information_(Last_5_Years).shp\")\n",
    "\n",
    "    ## Data Meta data - throw exception if not input\n",
    "    data_name = \"Crash Infromation (Last 5 Years)\"\n",
    "    data_summary = \"Crashes are recorded in the Integrated Road Information System (IRIS) and are provided for information only. The dataset provided is of crashes over the past 5 calendar years.\"\n",
    "    link_to_data = \"https://catalogue.data.wa.gov.au/dataset/mrwa-crash-information-last-5-years-\"\n",
    "    data_retieved = \"2022-03-13\"\n",
    "    date_updated = \"2020-11-11\"\n",
    "    published_by = \"Main Roads Western Australia\"\n",
    "    data_license = \"Creative Commons Attribution 4.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.import_data:\n",
    "    speed_gdf = geopandas.read_file(\"./data/Legal_Speed_Limits.shp\")\n",
    "\n",
    "    ## Data Meta data - throw exception if not input\n",
    "    data_name = \"Crash Infromation (Last 5 Years)\"\n",
    "    data_summary = \"Crashes are recorded in the Integrated Road Information System (IRIS) and are provided for information only. The dataset provided is of crashes over the past 5 calendar years.\"\n",
    "    link_to_data = \"https://catalogue.data.wa.gov.au/dataset/mrwa-crash-information-last-5-years-\"\n",
    "    data_retieved = \"2022-03-13\"\n",
    "    date_updated = \"2020-11-11\"\n",
    "    published_by = \"Main Roads Western Australia\"\n",
    "    data_license = \"Creative Commons Attribution 4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132375, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(67182, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Revie Data Shapes \n",
    "crash_gdf.shape\n",
    "speed_gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Merge \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\data_warehousing.venv\\lib\\site-packages\\geopandas\\array.py:341: UserWarning: Geometry is in a geographic CRS. Results from 'sjoin_nearest' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID_left', 'ACC_ID', 'ROAD_NO', 'ROAD_NAME_left', 'COMMON_ROA',\n",
       "       'CWAY', 'SLK', 'INTERSECTI', 'INTERSEC_1', 'LONGITUDE', 'LATITUDE',\n",
       "       'CRASH_DATE', 'CRASH_TIME', 'ACCIDENT_T', 'SEVERITY', 'EVENT_NATU',\n",
       "       'EVENT_TYPE', 'TOTAL_BIKE', 'TOTAL_TRUC', 'TOTAL_HEAV', 'TOTAL_MOTO',\n",
       "       'TOTAL_OTHE', 'TOTAL_PEDE', 'geometry', 'index_right', 'OBJECTID_right',\n",
       "       'ROAD', 'ROAD_NAME_right', 'COMMON_USA', 'START_SLK', 'END_SLK', 'CWY',\n",
       "       'START_TRUE', 'END_TRUE_D', 'NETWORK_TY', 'RA_NO', 'RA_NAME', 'LG_NO',\n",
       "       'LG_NAME', 'SPEED_LIMI', 'ROUTE_NE_I', 'GEOLOCSTLe'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_left</th>\n",
       "      <th>ACC_ID</th>\n",
       "      <th>ROAD_NO</th>\n",
       "      <th>ROAD_NAME_left</th>\n",
       "      <th>COMMON_ROA</th>\n",
       "      <th>CWAY</th>\n",
       "      <th>SLK</th>\n",
       "      <th>INTERSECTI</th>\n",
       "      <th>INTERSEC_1</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>...</th>\n",
       "      <th>START_TRUE</th>\n",
       "      <th>END_TRUE_D</th>\n",
       "      <th>NETWORK_TY</th>\n",
       "      <th>RA_NO</th>\n",
       "      <th>RA_NAME</th>\n",
       "      <th>LG_NO</th>\n",
       "      <th>LG_NAME</th>\n",
       "      <th>SPEED_LIMI</th>\n",
       "      <th>ROUTE_NE_I</th>\n",
       "      <th>GEOLOCSTLe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33989666</td>\n",
       "      <td>9517411</td>\n",
       "      <td>1240038</td>\n",
       "      <td>William St</td>\n",
       "      <td>William St</td>\n",
       "      <td>S</td>\n",
       "      <td>1.020</td>\n",
       "      <td>050522</td>\n",
       "      <td>William St &amp; Hay St</td>\n",
       "      <td>115.857020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.53</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>07</td>\n",
       "      <td>Metropolitan</td>\n",
       "      <td>124</td>\n",
       "      <td>Perth (C)</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>246091.0</td>\n",
       "      <td>0.014601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33989667</td>\n",
       "      <td>9517424</td>\n",
       "      <td>1260096</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>S</td>\n",
       "      <td>0.130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.875138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>07</td>\n",
       "      <td>Metropolitan</td>\n",
       "      <td>126</td>\n",
       "      <td>South Perth (C)</td>\n",
       "      <td>50km/h applies in built up areas or 110km/h ou...</td>\n",
       "      <td>219527.0</td>\n",
       "      <td>0.010143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33989668</td>\n",
       "      <td>9517437</td>\n",
       "      <td>1040027</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.942474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.61</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>07</td>\n",
       "      <td>Metropolitan</td>\n",
       "      <td>104</td>\n",
       "      <td>Gosnells (C)</td>\n",
       "      <td>60km/h</td>\n",
       "      <td>201001.0</td>\n",
       "      <td>0.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33989669</td>\n",
       "      <td>9517443</td>\n",
       "      <td>H005</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>6.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.937801</td>\n",
       "      <td>...</td>\n",
       "      <td>1.83</td>\n",
       "      <td>6.77</td>\n",
       "      <td>State Road</td>\n",
       "      <td>07</td>\n",
       "      <td>Metropolitan</td>\n",
       "      <td>113</td>\n",
       "      <td>Belmont (C)</td>\n",
       "      <td>60km/h</td>\n",
       "      <td>247651.0</td>\n",
       "      <td>0.049442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33989670</td>\n",
       "      <td>9517460</td>\n",
       "      <td>H032</td>\n",
       "      <td>South St</td>\n",
       "      <td>South St</td>\n",
       "      <td>L</td>\n",
       "      <td>7.76</td>\n",
       "      <td>047055</td>\n",
       "      <td>South St &amp; Murdoch Dr</td>\n",
       "      <td>115.842203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.89</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>07</td>\n",
       "      <td>Metropolitan</td>\n",
       "      <td>119</td>\n",
       "      <td>Melville (C)</td>\n",
       "      <td>70km/h</td>\n",
       "      <td>230074.0</td>\n",
       "      <td>0.035884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID_left   ACC_ID  ROAD_NO     ROAD_NAME_left         COMMON_ROA CWAY  \\\n",
       "0       33989666  9517411  1240038         William St         William St    S   \n",
       "1       33989667  9517424  1260096         Henning Cr         Henning Cr    S   \n",
       "2       33989668  9517437  1040027         Amherst Rd         Amherst Rd    S   \n",
       "3       33989669  9517443     H005  Great Eastern Hwy  Great Eastern Hwy    L   \n",
       "4       33989670  9517460     H032           South St           South St    L   \n",
       "\n",
       "     SLK INTERSECTI             INTERSEC_1   LONGITUDE  ...  START_TRUE  \\\n",
       "0  1.020     050522    William St & Hay St  115.857020  ...        0.00   \n",
       "1  0.130       None                   None  115.875138  ...        0.00   \n",
       "2  0.090       None                   None  115.942474  ...        0.00   \n",
       "3   6.13       None                   None  115.937801  ...        1.83   \n",
       "4   7.76     047055  South St & Murdoch Dr  115.842203  ...        0.00   \n",
       "\n",
       "  END_TRUE_D  NETWORK_TY RA_NO       RA_NAME LG_NO          LG_NAME  \\\n",
       "0       1.53  Local Road    07  Metropolitan   124        Perth (C)   \n",
       "1       0.96  Local Road    07  Metropolitan   126  South Perth (C)   \n",
       "2       2.61  Local Road    07  Metropolitan   104     Gosnells (C)   \n",
       "3       6.77  State Road    07  Metropolitan   113      Belmont (C)   \n",
       "4       3.89  Local Road    07  Metropolitan   119     Melville (C)   \n",
       "\n",
       "                                          SPEED_LIMI  ROUTE_NE_I  GEOLOCSTLe  \n",
       "0                                             40km/h    246091.0    0.014601  \n",
       "1  50km/h applies in built up areas or 110km/h ou...    219527.0    0.010143  \n",
       "2                                             60km/h    201001.0    0.024912  \n",
       "3                                             60km/h    247651.0    0.049442  \n",
       "4                                             70km/h    230074.0    0.035884  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.import_data:\n",
    "    geo_merged_gdf = geopandas.sjoin_nearest(left_df= crash_gdf, right_df=speed_gdf, how=\"left\" , max_distance=5)\n",
    "    geo_merged_gdf.columns\n",
    "    geo_merged_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: Enrich Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Static Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, tavg, tmin, tmax, prcp, snow, wdir, wspd, wpgt, pres, tsun]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.import_data:\n",
    "    ## Import bulk weather data\n",
    "    perth_weather_gdf = pd.read_csv(\"./data/Weather_Data_Belmot_94610.csv\")\n",
    "\n",
    "    ## Ensure not nulls in data\n",
    "    pattern = perth_weather_gdf.date.isna()\n",
    "    perth_weather_gdf[pattern]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix data formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.import_data:\n",
    "    ## Fix Date \n",
    "    fixed_dates = []\n",
    "\n",
    "    for date in perth_weather_gdf.date:\n",
    "        ## Split date into parts \n",
    "        ar = str(date).split(\"/\")\n",
    "        ## Add string padding \n",
    "        ar = [string.rjust(2, \"0\") for string in ar]\n",
    "        ## Rejoin dates \n",
    "        out_string = \"/\".join(ar)\n",
    "        fixed_dates.append(out_string)\n",
    "\n",
    "    perth_weather_gdf.date = fixed_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OBJECTID_left', 'ACC_ID', 'ROAD_NO', 'ROAD_NAME_left', 'COMMON_ROA',\n",
       "       'CWAY', 'SLK', 'INTERSECTI', 'INTERSEC_1', 'LONGITUDE', 'LATITUDE',\n",
       "       'CRASH_DATE', 'CRASH_TIME', 'ACCIDENT_T', 'SEVERITY', 'EVENT_NATU',\n",
       "       'EVENT_TYPE', 'TOTAL_BIKE', 'TOTAL_TRUC', 'TOTAL_HEAV', 'TOTAL_MOTO',\n",
       "       'TOTAL_OTHE', 'TOTAL_PEDE', 'geometry', 'index_right', 'OBJECTID_right',\n",
       "       'ROAD', 'ROAD_NAME_right', 'COMMON_USA', 'START_SLK', 'END_SLK', 'CWY',\n",
       "       'START_TRUE', 'END_TRUE_D', 'NETWORK_TY', 'RA_NO', 'RA_NAME', 'LG_NO',\n",
       "       'LG_NAME', 'SPEED_LIMI', 'ROUTE_NE_I', 'GEOLOCSTLe', 'date', 'tavg',\n",
       "       'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_left</th>\n",
       "      <th>ACC_ID</th>\n",
       "      <th>ROAD_NO</th>\n",
       "      <th>ROAD_NAME_left</th>\n",
       "      <th>COMMON_ROA</th>\n",
       "      <th>CWAY</th>\n",
       "      <th>SLK</th>\n",
       "      <th>INTERSECTI</th>\n",
       "      <th>INTERSEC_1</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>...</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>wpgt</th>\n",
       "      <th>pres</th>\n",
       "      <th>tsun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33989666</td>\n",
       "      <td>9517411</td>\n",
       "      <td>1240038</td>\n",
       "      <td>William St</td>\n",
       "      <td>William St</td>\n",
       "      <td>S</td>\n",
       "      <td>1.020</td>\n",
       "      <td>050522</td>\n",
       "      <td>William St &amp; Hay St</td>\n",
       "      <td>115.857020</td>\n",
       "      <td>...</td>\n",
       "      <td>18.3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33989667</td>\n",
       "      <td>9517424</td>\n",
       "      <td>1260096</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>S</td>\n",
       "      <td>0.130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.875138</td>\n",
       "      <td>...</td>\n",
       "      <td>21.1</td>\n",
       "      <td>14.5</td>\n",
       "      <td>27.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1013.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33989668</td>\n",
       "      <td>9517437</td>\n",
       "      <td>1040027</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.942474</td>\n",
       "      <td>...</td>\n",
       "      <td>21.3</td>\n",
       "      <td>20.1</td>\n",
       "      <td>27.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>230.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33989669</td>\n",
       "      <td>9517443</td>\n",
       "      <td>H005</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>6.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.937801</td>\n",
       "      <td>...</td>\n",
       "      <td>26.3</td>\n",
       "      <td>21.4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33989670</td>\n",
       "      <td>9517460</td>\n",
       "      <td>H032</td>\n",
       "      <td>South St</td>\n",
       "      <td>South St</td>\n",
       "      <td>L</td>\n",
       "      <td>7.76</td>\n",
       "      <td>047055</td>\n",
       "      <td>South St &amp; Murdoch Dr</td>\n",
       "      <td>115.842203</td>\n",
       "      <td>...</td>\n",
       "      <td>23.9</td>\n",
       "      <td>10.2</td>\n",
       "      <td>32.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>163.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1020.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID_left   ACC_ID  ROAD_NO     ROAD_NAME_left         COMMON_ROA CWAY  \\\n",
       "0       33989666  9517411  1240038         William St         William St    S   \n",
       "1       33989667  9517424  1260096         Henning Cr         Henning Cr    S   \n",
       "2       33989668  9517437  1040027         Amherst Rd         Amherst Rd    S   \n",
       "3       33989669  9517443     H005  Great Eastern Hwy  Great Eastern Hwy    L   \n",
       "4       33989670  9517460     H032           South St           South St    L   \n",
       "\n",
       "     SLK INTERSECTI             INTERSEC_1   LONGITUDE  ...  tavg  tmin  tmax  \\\n",
       "0  1.020     050522    William St & Hay St  115.857020  ...  18.3  13.5  24.9   \n",
       "1  0.130       None                   None  115.875138  ...  21.1  14.5  27.3   \n",
       "2  0.090       None                   None  115.942474  ...  21.3  20.1  27.2   \n",
       "3   6.13       None                   None  115.937801  ...  26.3  21.4  35.0   \n",
       "4   7.76     047055  South St & Murdoch Dr  115.842203  ...  23.9  10.2  32.1   \n",
       "\n",
       "  prcp snow   wdir  wspd  wpgt    pres  tsun  \n",
       "0  1.4  NaN  203.0  20.4   NaN     NaN   NaN  \n",
       "1  0.0  NaN  188.0  13.3   NaN  1013.9   NaN  \n",
       "2  0.0  NaN  230.0  21.1   NaN  1012.1   NaN  \n",
       "3  0.0  NaN  141.0  14.7   NaN  1009.5   NaN  \n",
       "4  0.0  NaN  163.0  13.7   NaN  1020.1   NaN  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.import_data:\n",
    "    ## Merge data on date\n",
    "    merged_gdf = geo_merged_gdf.merge(perth_weather_gdf , how=\"left\" , left_on=\"CRASH_DATE\" , right_on=\"date\") \n",
    "\n",
    "    ## Review data\n",
    "    merged_gdf.columns\n",
    "    merged_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill with station Id with default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>CRASH_DATE</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132533</th>\n",
       "      <td>94610</td>\n",
       "      <td>11/12/2020</td>\n",
       "      <td>11/12/2020</td>\n",
       "      <td>26.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>32.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132534</th>\n",
       "      <td>94610</td>\n",
       "      <td>22/01/2017</td>\n",
       "      <td>22/01/2017</td>\n",
       "      <td>19.3</td>\n",
       "      <td>15.8</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1010.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132535</th>\n",
       "      <td>94610</td>\n",
       "      <td>17/01/2021</td>\n",
       "      <td>17/01/2021</td>\n",
       "      <td>29.3</td>\n",
       "      <td>18.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132536</th>\n",
       "      <td>94610</td>\n",
       "      <td>10/08/2018</td>\n",
       "      <td>10/08/2018</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1026.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132537</th>\n",
       "      <td>94610</td>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>16.1</td>\n",
       "      <td>7.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1019.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_id        date  CRASH_DATE  tavg  tmin  tmax  prcp    pres\n",
       "132533      94610  11/12/2020  11/12/2020  26.1  23.7  32.3   0.0  1006.1\n",
       "132534      94610  22/01/2017  22/01/2017  19.3  15.8  26.3   0.0  1010.3\n",
       "132535      94610  17/01/2021  17/01/2021  29.3  18.7  36.1   0.0  1014.4\n",
       "132536      94610  10/08/2018  10/08/2018   8.7   2.5  15.0   9.2  1026.3\n",
       "132537      94610  03/09/2020  03/09/2020  16.1   7.2  20.0   2.2  1019.7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## Add in weather station id \n",
    "    merged_gdf[\"station_id\"] =  [\"94610\" for x in range(len(merged_gdf))]\n",
    "\n",
    "    ## Ensure weather data \n",
    "    merged_gdf[[ 'station_id', \"date\", \"CRASH_DATE\" , 'tavg','tmin', 'tmax', 'prcp','pres', ]].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich Data: Get Weather Data From API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment out to run\n",
    "# !pip install meteostat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to get weather data from crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.import_data:\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import matplotlib.pyplot as plt\n",
    "    from meteostat import Point, Daily, Stations, Hourly\n",
    "    from datetime import datetime\n",
    "\n",
    "    def get_start_end_times(date_string :str, time: float):\n",
    "        ## Get metrics from strings\n",
    "        split_date = date_string.split('/')\n",
    "        day = int(split_date[0])\n",
    "        month = int(split_date[1])\n",
    "        year = int(split_date[2])\n",
    "\n",
    "        hour = round(time/100)\n",
    "    \n",
    "        # Set time period\n",
    "        start = datetime(year, month, day, hour)\n",
    "        end = datetime(year, month, day, hour )\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    def get_start_end_dates(date_string :str):\n",
    "        ## Get metrics from strings\n",
    "        split_date = date_string.split('/')\n",
    "        day = int(split_date[0])\n",
    "        month = int(split_date[1])\n",
    "        year = int(split_date[2])\n",
    "        # Set time period\n",
    "        start = datetime(year, month, day)\n",
    "        end = datetime(year, month, day)\n",
    "        return start, end    \n",
    "\n",
    "    def get_weather_stn(x:float , y:float):\n",
    "        # Get nearby weather stations\n",
    "        stations = Stations()\n",
    "        stations = stations.nearby(x , y)\n",
    "        station = stations.fetch(1)\n",
    "        return station\n",
    "\n",
    "    def get_weather_data_hourly(station_id , start , end ):\n",
    "        # Get hourly data\n",
    "        data = Hourly(station_id ,start, end)\n",
    "        try:\n",
    "            data = data.fetch()\n",
    "            if len(data) == 0:\n",
    "                data.loc[0] = np.nan\n",
    "            return data\n",
    "        except Exception as e: print(e)\n",
    "\n",
    "    def get_weather_data_daily(station_id , start , end ):\n",
    "        # Get hourly data\n",
    "        data = Daily(station_id ,start, end)\n",
    "        try:\n",
    "            data = data.fetch()\n",
    "            if len(data) == 0:\n",
    "                data.loc[0] = np.nan\n",
    "            return data\n",
    "        except Exception as e: print(e)\n",
    "\n",
    "    def get_weather_from_crash(x , y, date_string :str, time_string: str , sleep = True , daily = True):\n",
    "        ## Ensure it does not overload the API\n",
    "        if sleep:\n",
    "            time.sleep(0.25)\n",
    "        ## Get the Weather Station Meta Data\n",
    "        station = get_weather_stn(float(x) , float(y))\n",
    "        ## Get the station id\n",
    "        stn_id = station.wmo[0]\n",
    "        ## Fetch the Weather data at this station at this time\n",
    "        if daily:\n",
    "            ## Get the start and end times from strings\n",
    "            start , end = get_start_end_dates(date_string)\n",
    "            weather_df = get_weather_data_daily(stn_id , start, end)\n",
    "        else:\n",
    "            ## Get the start and end times from strings\n",
    "            start , end = get_start_end_times(date_string , time_string)\n",
    "            weather_df = get_weather_data_hourly(stn_id , start, end)\n",
    "        return station , weather_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Weather data for cells and create matching dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data for Daily Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## Create dataframes for weather data \n",
    "    station_dfs = pd.DataFrame()\n",
    "    weather_dfs = pd.DataFrame()\n",
    "\n",
    "    ## iterate through subset of data\n",
    "    for i , row in merged_gdf.tail(10).iterrows():\n",
    "        ## Get data from rows\n",
    "        x = row.LATITUDE\n",
    "        y = row.LONGITUDE\n",
    "        date_string = row.CRASH_DATE\n",
    "        time_string = row.CRASH_TIME\n",
    "\n",
    "        ## Get weather data and station from API and input data \n",
    "        station_df , weather_df = get_weather_from_crash(x , y, date_string , time_string )\n",
    "\n",
    "        ## Insert values \n",
    "        for col in weather_df.columns:\n",
    "            merged_gdf.loc[i, col] = weather_df[col].values    \n",
    "        merged_gdf.loc[i, \"weather_id\"] = str(station_df.index.values[0] +\"_\"+ date_string)\n",
    "        merged_gdf.loc[i, \"station_id\"] = str(station_df.index.values[0])\n",
    "\n",
    "        ## Join the data frames \n",
    "        station_dfs = pd.concat([station_dfs, station_df])\n",
    "        weather_dfs = pd.concat([weather_dfs, weather_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>weather_id</th>\n",
       "      <th>date</th>\n",
       "      <th>CRASH_DATE</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>pres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132533</th>\n",
       "      <td>94608</td>\n",
       "      <td>94608_11/12/2020</td>\n",
       "      <td>11/12/2020</td>\n",
       "      <td>11/12/2020</td>\n",
       "      <td>28.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132534</th>\n",
       "      <td>95612</td>\n",
       "      <td>95612_22/01/2017</td>\n",
       "      <td>22/01/2017</td>\n",
       "      <td>22/01/2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132535</th>\n",
       "      <td>94614</td>\n",
       "      <td>94614_17/01/2021</td>\n",
       "      <td>17/01/2021</td>\n",
       "      <td>17/01/2021</td>\n",
       "      <td>28.3</td>\n",
       "      <td>19.9</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132536</th>\n",
       "      <td>94608</td>\n",
       "      <td>94608_10/08/2018</td>\n",
       "      <td>10/08/2018</td>\n",
       "      <td>10/08/2018</td>\n",
       "      <td>9.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1026.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132537</th>\n",
       "      <td>94605</td>\n",
       "      <td>94605_03/09/2020</td>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_id        weather_id        date  CRASH_DATE  tavg  tmin  tmax  \\\n",
       "132533      94608  94608_11/12/2020  11/12/2020  11/12/2020  28.1  23.1  33.1   \n",
       "132534      95612  95612_22/01/2017  22/01/2017  22/01/2017   NaN  16.0  25.5   \n",
       "132535      94614  94614_17/01/2021  17/01/2021  17/01/2021  28.3  19.9  36.3   \n",
       "132536      94608  94608_10/08/2018  10/08/2018  10/08/2018   9.4   3.2  14.8   \n",
       "132537      94605  94605_03/09/2020  03/09/2020  03/09/2020   NaN  14.4  19.6   \n",
       "\n",
       "        prcp    pres  \n",
       "132533   0.0  1006.1  \n",
       "132534   0.0     NaN  \n",
       "132535   0.0  1014.5  \n",
       "132536  11.2  1026.1  \n",
       "132537   1.0     NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Review Weather Data has merged correctly \n",
    "merged_gdf[['station_id', \"weather_id\", \"date\", \"CRASH_DATE\" , 'tavg', 'tmin', 'tmax', 'prcp','pres', ]].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Perth Weather Station Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>wmo</th>\n",
       "      <th>icao</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>hourly_start</th>\n",
       "      <th>hourly_end</th>\n",
       "      <th>daily_start</th>\n",
       "      <th>daily_end</th>\n",
       "      <th>monthly_start</th>\n",
       "      <th>monthly_end</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95604</td>\n",
       "      <td>Gosnells</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95604</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-02</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>8355.013929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94608</td>\n",
       "      <td>Perth</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94608</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.9</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5095.812224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95607</td>\n",
       "      <td>Garden Island</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95607</td>\n",
       "      <td>YGAD</td>\n",
       "      <td>-32.2</td>\n",
       "      <td>115.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>8562.153167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95612</td>\n",
       "      <td>Gingin Airport</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95612</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.5</td>\n",
       "      <td>115.9</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>23921.448614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94614</td>\n",
       "      <td>Swanbourne</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94614</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>115.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>7772.591528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94605</td>\n",
       "      <td>Mandurah</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94605</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>115.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6743.511195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            name country region    wmo  icao  latitude  longitude  \\\n",
       "0  95604        Gosnells      AU     WA  95604  <NA>     -32.0      116.0   \n",
       "1  94608           Perth      AU     WA  94608  <NA>     -31.9      115.9   \n",
       "2  95607   Garden Island      AU     WA  95607  YGAD     -32.2      115.7   \n",
       "3  95612  Gingin Airport      AU     WA  95612  <NA>     -31.5      115.9   \n",
       "4  94614      Swanbourne      AU     WA  94614  <NA>     -32.0      115.8   \n",
       "5  94605        Mandurah      AU     WA  94605  <NA>     -32.5      115.7   \n",
       "\n",
       "   elevation         timezone hourly_start hourly_end daily_start  daily_end  \\\n",
       "0       10.0  Australia/Perth          NaT        NaT  1750-02-02 2022-05-01   \n",
       "1       25.0  Australia/Perth          NaT        NaT  1750-02-01 2022-05-05   \n",
       "2       15.0  Australia/Perth          NaT        NaT  1750-02-01 2022-05-04   \n",
       "3       74.0  Australia/Perth          NaT        NaT  1750-02-01 2022-05-04   \n",
       "4       20.0  Australia/Perth          NaT        NaT  1750-02-01 2022-05-04   \n",
       "5       21.0  Australia/Perth          NaT        NaT  1750-02-01 2022-05-04   \n",
       "\n",
       "  monthly_start monthly_end      distance  \n",
       "0    1961-01-01  2020-01-01   8355.013929  \n",
       "1    1993-01-01  2022-01-01   5095.812224  \n",
       "2    2002-01-01  2022-01-01   8562.153167  \n",
       "3    1996-01-01  2022-01-01  23921.448614  \n",
       "4    1993-01-01  2022-01-01   7772.591528  \n",
       "5    2001-01-01  2022-01-01   6743.511195  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## Drop duplicate stations \n",
    "    station_dfs.drop_duplicates(subset=['name'] , inplace=True)\n",
    "    station_dfs.reset_index(inplace=True) \n",
    "    station_dfs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich Data: Add Extra Data Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>quarter</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>015</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>020</td>\n",
       "      <td>Monday</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>008</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>023</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year month  day day_of_week  quarter        date\n",
       "0  2017    03  015   Wednesday        1  2017-03-15\n",
       "1  2017    03  024      Friday        1  2017-03-24\n",
       "2  2017    02  020      Monday        1  2017-02-20\n",
       "3  2017    03  008   Wednesday        1  2017-03-08\n",
       "4  2017    02  023    Thursday        1  2017-02-23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    #is_peak_hour\n",
    "    def check_for_peak(time: float):\n",
    "        if 730 < time < 930:\n",
    "            return  1\n",
    "        elif 1600 < time < 1800:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    merged_gdf[\"is_peak_hour\"] = [check_for_peak(time) for time in merged_gdf[\"CRASH_TIME\"] ]\n",
    "\n",
    "    ## Create Date Variables\n",
    "    import datetime as dt\n",
    "\n",
    "    ## Converts string to datetime stamp \n",
    "    def convert_date_stamp(date: str):\n",
    "        d_split = list(map(int , date.split(\"/\")))\n",
    "        dt_date = dt.date( d_split[2] , d_split[1],  d_split[0],  )\n",
    "        date_stamp = pd.Timestamp(dt_date)\n",
    "        return date_stamp\n",
    "\n",
    "    ## Run this function to get datatime \n",
    "    date_stamps = [convert_date_stamp(date) for date in merged_gdf[\"CRASH_DATE\"] ]\n",
    "\n",
    "    ## Extract and input data values \n",
    "    merged_gdf[\"quarter\"] = [stamp.quarter  for stamp in date_stamps]\n",
    "    merged_gdf[\"year\"]  = [stamp.year  for stamp in date_stamps]\n",
    "    merged_gdf[\"month\"]  = [str(stamp.month).rjust(2, \"0\")  for stamp in date_stamps]\n",
    "    merged_gdf[\"day\"]  = [  str(stamp.day).rjust(3, \"0\")  for stamp in date_stamps]\n",
    "    merged_gdf[\"day_of_week\"]  = [stamp.day_name()  for stamp in date_stamps]\n",
    "    merged_gdf[\"date\"]  = [stamp.isoformat().split(\"T\")[0] for stamp in date_stamps]\n",
    "\n",
    "    ## Review insersion \n",
    "    merged_gdf[[\"year\", \"month\", \"day\" , \"day_of_week\", \"quarter\"  , \"date\"]].head(5)\n",
    "\n",
    "    # distance \n",
    "    merged_gdf[\"distance\"] = [ None for distance in merged_gdf[\"station_id\"] ]\n",
    "\n",
    "    ## Create generated keys \n",
    "    merged_gdf[\"weather_id\"] = [str(stn) + \"_\" + str(date) for stn , date in merged_gdf[[  \"station_id\" ,  \"date\" ]].values]\n",
    "\n",
    "    #is_intersection\n",
    "    merged_gdf[\"is_intersection\"] = [ 1 if isnull else 0 for isnull in merged_gdf[\"INTERSECTI\"].isnull() ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: Create Natural Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## location_id\n",
    "    merged_gdf[\"location_id\"] = [str(road) + \"_\" + str(inter) for road  , inter in merged_gdf[[ \"ROAD_NO\", \"INTERSECTI\"  ]].values]\n",
    "\n",
    "    ## event_id\n",
    "    merged_gdf[\"event_id\"] = range(len(merged_gdf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns with Dictionary Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_left</th>\n",
       "      <th>crash_id</th>\n",
       "      <th>road_id</th>\n",
       "      <th>road_name</th>\n",
       "      <th>common_road_name</th>\n",
       "      <th>cway</th>\n",
       "      <th>SLK</th>\n",
       "      <th>intersection_no</th>\n",
       "      <th>intersection_desc</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>is_peak_hour</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>distance</th>\n",
       "      <th>is_intersection</th>\n",
       "      <th>location_id</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33989666</td>\n",
       "      <td>9517411</td>\n",
       "      <td>1240038</td>\n",
       "      <td>William St</td>\n",
       "      <td>William St</td>\n",
       "      <td>S</td>\n",
       "      <td>1.020</td>\n",
       "      <td>050522</td>\n",
       "      <td>William St &amp; Hay St</td>\n",
       "      <td>115.857020</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>015</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1240038_050522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33989667</td>\n",
       "      <td>9517424</td>\n",
       "      <td>1260096</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>S</td>\n",
       "      <td>0.130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.875138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1260096_None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33989668</td>\n",
       "      <td>9517437</td>\n",
       "      <td>1040027</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.942474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>020</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1040027_None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33989669</td>\n",
       "      <td>9517443</td>\n",
       "      <td>H005</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>6.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.937801</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>008</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>H005_None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33989670</td>\n",
       "      <td>9517460</td>\n",
       "      <td>H032</td>\n",
       "      <td>South St</td>\n",
       "      <td>South St</td>\n",
       "      <td>L</td>\n",
       "      <td>7.76</td>\n",
       "      <td>047055</td>\n",
       "      <td>South St &amp; Murdoch Dr</td>\n",
       "      <td>115.842203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>023</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>H032_047055</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID_left  crash_id  road_id          road_name   common_road_name  \\\n",
       "0       33989666   9517411  1240038         William St         William St   \n",
       "1       33989667   9517424  1260096         Henning Cr         Henning Cr   \n",
       "2       33989668   9517437  1040027         Amherst Rd         Amherst Rd   \n",
       "3       33989669   9517443     H005  Great Eastern Hwy  Great Eastern Hwy   \n",
       "4       33989670   9517460     H032           South St           South St   \n",
       "\n",
       "  cway    SLK intersection_no      intersection_desc   longitude  ...  \\\n",
       "0    S  1.020          050522    William St & Hay St  115.857020  ...   \n",
       "1    S  0.130            None                   None  115.875138  ...   \n",
       "2    S  0.090            None                   None  115.942474  ...   \n",
       "3    L   6.13            None                   None  115.937801  ...   \n",
       "4    L   7.76          047055  South St & Murdoch Dr  115.842203  ...   \n",
       "\n",
       "   is_peak_hour quarter  year month  day day_of_week distance  \\\n",
       "0             1       1  2017    03  015   Wednesday     None   \n",
       "1             0       1  2017    03  024      Friday     None   \n",
       "2             0       1  2017    02  020      Monday     None   \n",
       "3             1       1  2017    03  008   Wednesday     None   \n",
       "4             0       1  2017    02  023    Thursday     None   \n",
       "\n",
       "   is_intersection     location_id  event_id  \n",
       "0                0  1240038_050522         0  \n",
       "1                1    1260096_None         1  \n",
       "2                1    1040027_None         2  \n",
       "3                1       H005_None         3  \n",
       "4                0     H032_047055         4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## Column naming dictionary \n",
    "    column_mapping = dict({\n",
    "    'ACC_ID' : \"crash_id\", \n",
    "    'ROAD_NO' : \"road_id\", \n",
    "    'ROAD_NAME_left' : \"road_name\", \n",
    "    'COMMON_ROA' : \"common_road_name\",\n",
    "    'CWAY' : \"cway\", \n",
    "    'INTERSECTI' : \"intersection_no\", \n",
    "    'INTERSEC_1' : \"intersection_desc\", \n",
    "    'LONGITUDE' : \"longitude\", \n",
    "    'LATITUDE' : \"latitude\",\n",
    "    'CRASH_TIME' : \"time_id\", \n",
    "    'SEVERITY' : \"severity\", \n",
    "    'EVENT_NATU' : \"event_nature\",\n",
    "    'EVENT_TYPE' : \"event_type\", \n",
    "    'TOTAL_BIKE' : \"total_bike_involved\", \n",
    "    'TOTAL_TRUC' : \"total_truck_involved\", \n",
    "    'TOTAL_HEAV' : \"total_heavy_truck_involved\", \n",
    "    'TOTAL_MOTO' : \"total_motor_cycle_involved\",\n",
    "    'TOTAL_OTHE' : \"total_other_vehicles_involved\", \n",
    "    'TOTAL_PEDE' : \"total_pedestrians_involve\", \n",
    "    'NETWORK_TY' : \"network_type\", \n",
    "    'RA_NO' : \"region_id\", \n",
    "    'RA_NAME' : \"region_name\", \n",
    "    'LG_NO' : \"lga_id\",\n",
    "    'LG_NAME' : \"local_gov_name\", \n",
    "    'SPEED_LIMI' : \"speed_limit\", \n",
    "    'date' : \"date_id\", \n",
    "    'tavg' : \"temp_ave\",\n",
    "    'tmin' : \"temp_min\", \n",
    "    'tmax' : \"temp_max\", \n",
    "    'prcp' : \"precipitation_mm\", \n",
    "    'wdir' : \"wind_direction\", \n",
    "    'wspd' : \"wind_speed\", \n",
    "    'pres' : \"pressure\", \n",
    "    'station_id' : \"station_id\",\n",
    "    })\n",
    "\n",
    "    def rename_columns(df: pd.DataFrame):\n",
    "        return df.rename(columns=column_mapping)\n",
    "\n",
    "    df = rename_columns(merged_gdf )\n",
    "    merged_df = pd.DataFrame(df)\n",
    "\n",
    "    ## Weather Stn Data Mapping \n",
    "    merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>wmo</th>\n",
       "      <th>icao</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>hourly_start_date</th>\n",
       "      <th>hourly_end_date</th>\n",
       "      <th>daily_start_date</th>\n",
       "      <th>daily_end_date</th>\n",
       "      <th>monthly_start_date</th>\n",
       "      <th>monthly_end_date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95604</td>\n",
       "      <td>Gosnells</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95604</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-02</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>8355.013929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94608</td>\n",
       "      <td>Perth</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94608</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.9</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5095.812224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95607</td>\n",
       "      <td>Garden Island</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95607</td>\n",
       "      <td>YGAD</td>\n",
       "      <td>-32.2</td>\n",
       "      <td>115.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>8562.153167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95612</td>\n",
       "      <td>Gingin Airport</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95612</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.5</td>\n",
       "      <td>115.9</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>23921.448614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94614</td>\n",
       "      <td>Swanbourne</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94614</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>115.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>7772.591528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94605</td>\n",
       "      <td>Mandurah</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94605</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>115.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6743.511195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id    station_name country region    wmo  icao  latitude  longitude  \\\n",
       "0      95604        Gosnells      AU     WA  95604  <NA>     -32.0      116.0   \n",
       "1      94608           Perth      AU     WA  94608  <NA>     -31.9      115.9   \n",
       "2      95607   Garden Island      AU     WA  95607  YGAD     -32.2      115.7   \n",
       "3      95612  Gingin Airport      AU     WA  95612  <NA>     -31.5      115.9   \n",
       "4      94614      Swanbourne      AU     WA  94614  <NA>     -32.0      115.8   \n",
       "5      94605        Mandurah      AU     WA  94605  <NA>     -32.5      115.7   \n",
       "\n",
       "   elevation         timezone hourly_start_date hourly_end_date  \\\n",
       "0       10.0  Australia/Perth               NaT             NaT   \n",
       "1       25.0  Australia/Perth               NaT             NaT   \n",
       "2       15.0  Australia/Perth               NaT             NaT   \n",
       "3       74.0  Australia/Perth               NaT             NaT   \n",
       "4       20.0  Australia/Perth               NaT             NaT   \n",
       "5       21.0  Australia/Perth               NaT             NaT   \n",
       "\n",
       "  daily_start_date daily_end_date monthly_start_date monthly_end_date  \\\n",
       "0       1750-02-02     2022-05-01         1961-01-01       2020-01-01   \n",
       "1       1750-02-01     2022-05-05         1993-01-01       2022-01-01   \n",
       "2       1750-02-01     2022-05-04         2002-01-01       2022-01-01   \n",
       "3       1750-02-01     2022-05-04         1996-01-01       2022-01-01   \n",
       "4       1750-02-01     2022-05-04         1993-01-01       2022-01-01   \n",
       "5       1750-02-01     2022-05-04         2001-01-01       2022-01-01   \n",
       "\n",
       "       distance  \n",
       "0   8355.013929  \n",
       "1   5095.812224  \n",
       "2   8562.153167  \n",
       "3  23921.448614  \n",
       "4   7772.591528  \n",
       "5   6743.511195  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    ## Renaming key value pairs \n",
    "    stn_rename = {\n",
    "        \"id\":\"station_id\",\n",
    "        \"name\":\"station_name\",\n",
    "        \"hourly_start\":\"hourly_start_date\",\n",
    "        \"hourly_end\":\"hourly_end_date\",\n",
    "        \"daily_start\":\"daily_start_date\",\n",
    "        \"daily_end\":\"daily_end_date\",\n",
    "        \"monthly_start\":\"monthly_start_date\",\n",
    "        \"monthly_end\":\"monthly_end_date\",\n",
    "    }\n",
    "\n",
    "    station_dfs.rename(columns=stn_rename, inplace=True)\n",
    "    station_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: Data Quality and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>wmo</th>\n",
       "      <th>icao</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>hourly_start_date</th>\n",
       "      <th>hourly_end_date</th>\n",
       "      <th>daily_start_date</th>\n",
       "      <th>daily_end_date</th>\n",
       "      <th>monthly_start_date</th>\n",
       "      <th>monthly_end_date</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95604</td>\n",
       "      <td>Gosnells</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95604</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-02</td>\n",
       "      <td>2022-05-01</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>8355.013929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94608</td>\n",
       "      <td>Perth</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94608</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.9</td>\n",
       "      <td>115.9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-05</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>5095.812224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95607</td>\n",
       "      <td>Garden Island</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95607</td>\n",
       "      <td>YGAD</td>\n",
       "      <td>-32.2</td>\n",
       "      <td>115.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>8562.153167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95612</td>\n",
       "      <td>Gingin Airport</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>95612</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-31.5</td>\n",
       "      <td>115.9</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1996-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>23921.448614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94614</td>\n",
       "      <td>Swanbourne</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94614</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>115.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>7772.591528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94605</td>\n",
       "      <td>Mandurah</td>\n",
       "      <td>AU</td>\n",
       "      <td>WA</td>\n",
       "      <td>94605</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>-32.5</td>\n",
       "      <td>115.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Australia/Perth</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1750-02-01</td>\n",
       "      <td>2022-05-04</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>6743.511195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id    station_name country region    wmo  icao  latitude  longitude  \\\n",
       "0      95604        Gosnells      AU     WA  95604  <NA>     -32.0      116.0   \n",
       "1      94608           Perth      AU     WA  94608  <NA>     -31.9      115.9   \n",
       "2      95607   Garden Island      AU     WA  95607  YGAD     -32.2      115.7   \n",
       "3      95612  Gingin Airport      AU     WA  95612  <NA>     -31.5      115.9   \n",
       "4      94614      Swanbourne      AU     WA  94614  <NA>     -32.0      115.8   \n",
       "5      94605        Mandurah      AU     WA  94605  <NA>     -32.5      115.7   \n",
       "\n",
       "   elevation         timezone hourly_start_date hourly_end_date  \\\n",
       "0       10.0  Australia/Perth               NaT             NaT   \n",
       "1       25.0  Australia/Perth               NaT             NaT   \n",
       "2       15.0  Australia/Perth               NaT             NaT   \n",
       "3       74.0  Australia/Perth               NaT             NaT   \n",
       "4       20.0  Australia/Perth               NaT             NaT   \n",
       "5       21.0  Australia/Perth               NaT             NaT   \n",
       "\n",
       "  daily_start_date daily_end_date monthly_start_date monthly_end_date  \\\n",
       "0       1750-02-02     2022-05-01         1961-01-01       2020-01-01   \n",
       "1       1750-02-01     2022-05-05         1993-01-01       2022-01-01   \n",
       "2       1750-02-01     2022-05-04         2002-01-01       2022-01-01   \n",
       "3       1750-02-01     2022-05-04         1996-01-01       2022-01-01   \n",
       "4       1750-02-01     2022-05-04         1993-01-01       2022-01-01   \n",
       "5       1750-02-01     2022-05-04         2001-01-01       2022-01-01   \n",
       "\n",
       "       distance  \n",
       "0   8355.013929  \n",
       "1   5095.812224  \n",
       "2   8562.153167  \n",
       "3  23921.448614  \n",
       "4   7772.591528  \n",
       "5   6743.511195  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_left</th>\n",
       "      <th>crash_id</th>\n",
       "      <th>road_id</th>\n",
       "      <th>road_name</th>\n",
       "      <th>common_road_name</th>\n",
       "      <th>cway</th>\n",
       "      <th>SLK</th>\n",
       "      <th>intersection_no</th>\n",
       "      <th>intersection_desc</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>is_peak_hour</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>distance</th>\n",
       "      <th>is_intersection</th>\n",
       "      <th>location_id</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33989666</td>\n",
       "      <td>9517411</td>\n",
       "      <td>1240038</td>\n",
       "      <td>William St</td>\n",
       "      <td>William St</td>\n",
       "      <td>S</td>\n",
       "      <td>1.020</td>\n",
       "      <td>050522</td>\n",
       "      <td>William St &amp; Hay St</td>\n",
       "      <td>115.857020</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>015</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1240038_050522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33989667</td>\n",
       "      <td>9517424</td>\n",
       "      <td>1260096</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>S</td>\n",
       "      <td>0.130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.875138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1260096_None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33989668</td>\n",
       "      <td>9517437</td>\n",
       "      <td>1040027</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.942474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>020</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1040027_None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33989669</td>\n",
       "      <td>9517443</td>\n",
       "      <td>H005</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>6.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.937801</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>008</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>H005_None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33989670</td>\n",
       "      <td>9517460</td>\n",
       "      <td>H032</td>\n",
       "      <td>South St</td>\n",
       "      <td>South St</td>\n",
       "      <td>L</td>\n",
       "      <td>7.76</td>\n",
       "      <td>047055</td>\n",
       "      <td>South St &amp; Murdoch Dr</td>\n",
       "      <td>115.842203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>023</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>H032_047055</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33989671</td>\n",
       "      <td>9517471</td>\n",
       "      <td>1313303</td>\n",
       "      <td>Grand Bvd</td>\n",
       "      <td>Grand Bvd</td>\n",
       "      <td>L</td>\n",
       "      <td>1.080</td>\n",
       "      <td>067855</td>\n",
       "      <td>Boas Av &amp; Grand Bvd</td>\n",
       "      <td>115.769956</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>008</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1313303_067855</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33989672</td>\n",
       "      <td>9517484</td>\n",
       "      <td>1180012</td>\n",
       "      <td>Elder Pl</td>\n",
       "      <td>Elder Pl</td>\n",
       "      <td>S</td>\n",
       "      <td>0.210</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.747164</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>027</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1180012_None</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33989673</td>\n",
       "      <td>9517492</td>\n",
       "      <td>H012</td>\n",
       "      <td>Leach Hwy</td>\n",
       "      <td>Leach Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>18.46</td>\n",
       "      <td>004456</td>\n",
       "      <td>Leach Hwy &amp; Welshpool Rd</td>\n",
       "      <td>115.929167</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>002</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>H012_004456</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33989674</td>\n",
       "      <td>9517498</td>\n",
       "      <td>8140653</td>\n",
       "      <td>Dampier Rd</td>\n",
       "      <td>Dampier Rd</td>\n",
       "      <td>L</td>\n",
       "      <td>0.000</td>\n",
       "      <td>008501</td>\n",
       "      <td>Millstream Rd &amp; Dampier Rd &amp; De Witt Rd</td>\n",
       "      <td>116.848394</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>010</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>8140653_008501</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33989675</td>\n",
       "      <td>9517504</td>\n",
       "      <td>1140265</td>\n",
       "      <td>Liege St</td>\n",
       "      <td>Liege St</td>\n",
       "      <td>S</td>\n",
       "      <td>0.600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.940769</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>026</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1140265_None</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID_left  crash_id  road_id          road_name   common_road_name  \\\n",
       "0       33989666   9517411  1240038         William St         William St   \n",
       "1       33989667   9517424  1260096         Henning Cr         Henning Cr   \n",
       "2       33989668   9517437  1040027         Amherst Rd         Amherst Rd   \n",
       "3       33989669   9517443     H005  Great Eastern Hwy  Great Eastern Hwy   \n",
       "4       33989670   9517460     H032           South St           South St   \n",
       "5       33989671   9517471  1313303          Grand Bvd          Grand Bvd   \n",
       "6       33989672   9517484  1180012           Elder Pl           Elder Pl   \n",
       "7       33989673   9517492     H012          Leach Hwy          Leach Hwy   \n",
       "8       33989674   9517498  8140653         Dampier Rd         Dampier Rd   \n",
       "9       33989675   9517504  1140265           Liege St           Liege St   \n",
       "\n",
       "  cway    SLK intersection_no                        intersection_desc  \\\n",
       "0    S  1.020          050522                      William St & Hay St   \n",
       "1    S  0.130            None                                     None   \n",
       "2    S  0.090            None                                     None   \n",
       "3    L   6.13            None                                     None   \n",
       "4    L   7.76          047055                    South St & Murdoch Dr   \n",
       "5    L  1.080          067855                      Boas Av & Grand Bvd   \n",
       "6    S  0.210            None                                     None   \n",
       "7    L  18.46          004456                 Leach Hwy & Welshpool Rd   \n",
       "8    L  0.000          008501  Millstream Rd & Dampier Rd & De Witt Rd   \n",
       "9    S  0.600            None                                     None   \n",
       "\n",
       "    longitude  ...  is_peak_hour quarter  year month  day day_of_week  \\\n",
       "0  115.857020  ...             1       1  2017    03  015   Wednesday   \n",
       "1  115.875138  ...             0       1  2017    03  024      Friday   \n",
       "2  115.942474  ...             0       1  2017    02  020      Monday   \n",
       "3  115.937801  ...             1       1  2017    03  008   Wednesday   \n",
       "4  115.842203  ...             0       1  2017    02  023    Thursday   \n",
       "5  115.769956  ...             0       1  2017    03  008   Wednesday   \n",
       "6  115.747164  ...             0       1  2017    02  027      Monday   \n",
       "7  115.929167  ...             1       1  2017    03  002    Thursday   \n",
       "8  116.848394  ...             1       1  2017    03  010      Friday   \n",
       "9  115.940769  ...             1       1  2017    02  026      Sunday   \n",
       "\n",
       "  distance  is_intersection     location_id  event_id  \n",
       "0     None                0  1240038_050522         0  \n",
       "1     None                1    1260096_None         1  \n",
       "2     None                1    1040027_None         2  \n",
       "3     None                1       H005_None         3  \n",
       "4     None                0     H032_047055         4  \n",
       "5     None                0  1313303_067855         5  \n",
       "6     None                1    1180012_None         6  \n",
       "7     None                0     H012_004456         7  \n",
       "8     None                0  8140653_008501         8  \n",
       "9     None                1    1140265_None         9  \n",
       "\n",
       "[10 rows x 65 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_left</th>\n",
       "      <th>crash_id</th>\n",
       "      <th>road_id</th>\n",
       "      <th>road_name</th>\n",
       "      <th>common_road_name</th>\n",
       "      <th>cway</th>\n",
       "      <th>SLK</th>\n",
       "      <th>intersection_no</th>\n",
       "      <th>intersection_desc</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>is_peak_hour</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>distance</th>\n",
       "      <th>is_intersection</th>\n",
       "      <th>location_id</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33989666</td>\n",
       "      <td>9517411</td>\n",
       "      <td>1240038</td>\n",
       "      <td>William St</td>\n",
       "      <td>William St</td>\n",
       "      <td>S</td>\n",
       "      <td>1.020</td>\n",
       "      <td>050522</td>\n",
       "      <td>William St &amp; Hay St</td>\n",
       "      <td>115.857020</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>015</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1240038_050522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33989667</td>\n",
       "      <td>9517424</td>\n",
       "      <td>1260096</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>Henning Cr</td>\n",
       "      <td>S</td>\n",
       "      <td>0.130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.875138</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1260096_None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33989668</td>\n",
       "      <td>9517437</td>\n",
       "      <td>1040027</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>Amherst Rd</td>\n",
       "      <td>S</td>\n",
       "      <td>0.090</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.942474</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>020</td>\n",
       "      <td>Monday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1040027_None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33989669</td>\n",
       "      <td>9517443</td>\n",
       "      <td>H005</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>Great Eastern Hwy</td>\n",
       "      <td>L</td>\n",
       "      <td>6.13</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.937801</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>03</td>\n",
       "      <td>008</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>H005_None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33989670</td>\n",
       "      <td>9517460</td>\n",
       "      <td>H032</td>\n",
       "      <td>South St</td>\n",
       "      <td>South St</td>\n",
       "      <td>L</td>\n",
       "      <td>7.76</td>\n",
       "      <td>047055</td>\n",
       "      <td>South St &amp; Murdoch Dr</td>\n",
       "      <td>115.842203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>02</td>\n",
       "      <td>023</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>H032_047055</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132533</th>\n",
       "      <td>34122036</td>\n",
       "      <td>10374579</td>\n",
       "      <td>H016</td>\n",
       "      <td>Mitchell Fwy</td>\n",
       "      <td>Mitchell Fwy</td>\n",
       "      <td>L</td>\n",
       "      <td>14.47</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.798060</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>011</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>H016_None</td>\n",
       "      <td>132533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132534</th>\n",
       "      <td>34122037</td>\n",
       "      <td>9483262</td>\n",
       "      <td>1105314</td>\n",
       "      <td>Dekle L</td>\n",
       "      <td>Dekle L</td>\n",
       "      <td>S</td>\n",
       "      <td>0.110</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.717791</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>01</td>\n",
       "      <td>022</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1105314_None</td>\n",
       "      <td>132534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132535</th>\n",
       "      <td>34122038</td>\n",
       "      <td>10391332</td>\n",
       "      <td>1280464</td>\n",
       "      <td>Sweetapple L</td>\n",
       "      <td>Sweetapple L</td>\n",
       "      <td>S</td>\n",
       "      <td>0.240</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.819402</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>01</td>\n",
       "      <td>017</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1280464_None</td>\n",
       "      <td>132535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132536</th>\n",
       "      <td>34122039</td>\n",
       "      <td>9843922</td>\n",
       "      <td>H016</td>\n",
       "      <td>Mitchell Fwy</td>\n",
       "      <td>Mitchell Fwy</td>\n",
       "      <td>L</td>\n",
       "      <td>24.88</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.769444</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>08</td>\n",
       "      <td>010</td>\n",
       "      <td>Friday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>H016_None</td>\n",
       "      <td>132536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132537</th>\n",
       "      <td>34122040</td>\n",
       "      <td>10296720</td>\n",
       "      <td>2121228</td>\n",
       "      <td>Boardwalk Bvd</td>\n",
       "      <td>Boardwalk Bvd</td>\n",
       "      <td>R</td>\n",
       "      <td>1.600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>115.676867</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>09</td>\n",
       "      <td>003</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2121228_None</td>\n",
       "      <td>132537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132538 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        OBJECTID_left  crash_id  road_id          road_name  \\\n",
       "0            33989666   9517411  1240038         William St   \n",
       "1            33989667   9517424  1260096         Henning Cr   \n",
       "2            33989668   9517437  1040027         Amherst Rd   \n",
       "3            33989669   9517443     H005  Great Eastern Hwy   \n",
       "4            33989670   9517460     H032           South St   \n",
       "...               ...       ...      ...                ...   \n",
       "132533       34122036  10374579     H016       Mitchell Fwy   \n",
       "132534       34122037   9483262  1105314            Dekle L   \n",
       "132535       34122038  10391332  1280464       Sweetapple L   \n",
       "132536       34122039   9843922     H016       Mitchell Fwy   \n",
       "132537       34122040  10296720  2121228      Boardwalk Bvd   \n",
       "\n",
       "         common_road_name cway    SLK intersection_no      intersection_desc  \\\n",
       "0              William St    S  1.020          050522    William St & Hay St   \n",
       "1              Henning Cr    S  0.130            None                   None   \n",
       "2              Amherst Rd    S  0.090            None                   None   \n",
       "3       Great Eastern Hwy    L   6.13            None                   None   \n",
       "4                South St    L   7.76          047055  South St & Murdoch Dr   \n",
       "...                   ...  ...    ...             ...                    ...   \n",
       "132533       Mitchell Fwy    L  14.47            None                   None   \n",
       "132534            Dekle L    S  0.110            None                   None   \n",
       "132535       Sweetapple L    S  0.240            None                   None   \n",
       "132536       Mitchell Fwy    L  24.88            None                   None   \n",
       "132537      Boardwalk Bvd    R  1.600            None                   None   \n",
       "\n",
       "         longitude  ...  is_peak_hour quarter  year month  day day_of_week  \\\n",
       "0       115.857020  ...             1       1  2017    03  015   Wednesday   \n",
       "1       115.875138  ...             0       1  2017    03  024      Friday   \n",
       "2       115.942474  ...             0       1  2017    02  020      Monday   \n",
       "3       115.937801  ...             1       1  2017    03  008   Wednesday   \n",
       "4       115.842203  ...             0       1  2017    02  023    Thursday   \n",
       "...            ...  ...           ...     ...   ...   ...  ...         ...   \n",
       "132533  115.798060  ...             0       4  2020    12  011      Friday   \n",
       "132534  115.717791  ...             0       1  2017    01  022      Sunday   \n",
       "132535  115.819402  ...             0       1  2021    01  017      Sunday   \n",
       "132536  115.769444  ...             1       3  2018    08  010      Friday   \n",
       "132537  115.676867  ...             0       3  2020    09  003    Thursday   \n",
       "\n",
       "       distance  is_intersection     location_id  event_id  \n",
       "0          None                0  1240038_050522         0  \n",
       "1          None                1    1260096_None         1  \n",
       "2          None                1    1040027_None         2  \n",
       "3          None                1       H005_None         3  \n",
       "4          None                0     H032_047055         4  \n",
       "...         ...              ...             ...       ...  \n",
       "132533     None                1       H016_None    132533  \n",
       "132534     None                1    1105314_None    132534  \n",
       "132535     None                1    1280464_None    132535  \n",
       "132536     None                1       H016_None    132536  \n",
       "132537     None                1    2121228_None    132537  \n",
       "\n",
       "[132538 rows x 65 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if functions_to_run.transform:\n",
    "    def replace_string(string):\n",
    "        station_dfs[string] = station_dfs[string].fillna(\"Null\")\n",
    "        station_dfs[string] = [stamp.isoformat().split(\"T\")[0] if str(stamp) != \"Null\" and type(stamp) != str else stamp for stamp in station_dfs[string]]\n",
    "\n",
    "    ## Replace 0s\n",
    "    station_dfs.replace(0 , None)\n",
    "\n",
    "    ## Replace None with Null strings \n",
    "    strings = [\"hourly_start_date\",\"hourly_end_date\",\"daily_start_date\",\"daily_end_date\",\"monthly_start_date\",\"monthly_end_date\"]\n",
    "    for string in strings:\n",
    "        replace_string(string)\n",
    "\n",
    "    ## Remove all ' from strings \n",
    "    def replace_string(string):\n",
    "        merged_df[string] = [s.replace(\"'\" , \" \") if s != None else None for s in merged_df[string]]\n",
    "\n",
    "    strings = [\"road_name\",\"common_road_name\",\"intersection_no\",  \"intersection_desc\" ]\n",
    "    for string in strings:\n",
    "        replace_string(string)\n",
    "\n",
    "    merged_df.head(10)\n",
    "\n",
    "    ## Clean Speed Variable \n",
    "    def clean_speed_vars(df:pd.DataFrame):\n",
    "        val_to_replace =   '50km/h applies in built up areas or 110km/h outside built up areas'\n",
    "        val_to_replace_with =   '110km/h or 50km/h built up areas'\n",
    "\n",
    "        df[\"speed_limit\"] = [val_to_replace_with  if speed == val_to_replace else speed for speed in df[\"speed_limit\"]]\n",
    "        return df\n",
    "\n",
    "    clean_speed_vars(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Null Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Nulls & Add to Meta Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.assignment_1_not_needed:\n",
    "    import math\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    ## Check for nulls in the dataframe\n",
    "    def review_nulls(df: pd.DataFrame):\n",
    "        if df.isnull().sum().sum() != 0:\n",
    "            na_df = (df.isnull().sum() / len(df)) * 100      \n",
    "            na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
    "            return pd.DataFrame({'Missing Ratio %' :na_df})\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    ## Create null plot\n",
    "    def plot_nas(missing_data: pd.DataFrame):\n",
    "            g = missing_data.plot(kind = \"barh\")\n",
    "            g.set_title(\"Ratio of Missing Data by Variable\", fontsize=18)\n",
    "            index = 0\n",
    "            for i, row in missing_data.iterrows():\n",
    "                txt = \"{1}%\".format(i, round(row[0] , 2))\n",
    "                loc = row[0]         \n",
    "                ## add annotations \n",
    "                g.text(\n",
    "                    loc + 3 , \n",
    "                    index  , \n",
    "                    txt ,  \n",
    "                    fontdict=dict(color='black', fontsize=12), \n",
    "                    horizontalalignment='center' \n",
    "                )\n",
    "                index += 1\n",
    "            plt.show()\n",
    "\n",
    "    ## Get Missing Data \n",
    "    missing_data = review_nulls(crash_gdf) \n",
    "    missing_data.head(5)  ## NOTE: this would be added to the meta data table \n",
    "    \n",
    "    if functions_to_run.assignment_1_not_needed:\n",
    "    ## Plot Missing Dta \n",
    "        if len(missing_data) != 0:\n",
    "            plot_nas(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.assignment_1_not_needed:\n",
    "    ## Duplicate Counts \n",
    "    len(crash_gdf[\"OBJECTID\"])-len(crash_gdf[\"OBJECTID\"].drop_duplicates())\n",
    "    len(crash_gdf[\"ACC_ID\"])-len(crash_gdf[\"ACC_ID\"].drop_duplicates())\n",
    "    len(crash_gdf)-len(crash_gdf.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.assignment_1_not_needed:\n",
    "    def review_dups(df: pd.DataFrame):\n",
    "        if len(df)-len(df.drop_duplicates()) != 0:\n",
    "            na_df = (df.duplicated().sum() / len(df)) * 100      \n",
    "            na_df = na_df.drop(na_df[na_df == 0].index).sort_values(ascending=False)\n",
    "            return pd.DataFrame({'Missing Ratio %' :na_df})\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "\n",
    "    def plot_dups(missing_data: pd.DataFrame):\n",
    "            ## Plot of training testing split \n",
    "            g = missing_data.plot(kind = \"barh\")\n",
    "            g.set_title(\"Ratio of Missing Data by Variable\", fontsize=18)\n",
    "            index = 0\n",
    "            for i, row in missing_data.iterrows():\n",
    "                txt = \"{1}%\".format(i, round(row[0] , 2))\n",
    "                loc = row[0]         \n",
    "                g.text(\n",
    "                    loc + 3 , \n",
    "                    index  , \n",
    "                    txt ,  \n",
    "                    fontdict=dict(color='black', fontsize=12), \n",
    "                    horizontalalignment='center' \n",
    "                )\n",
    "                index += 1\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    dup_data = review_dups(crash_gdf)\n",
    "    if len(dup_data ) != 0:\n",
    "        plot_dups(dup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python SQL Server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment out to run\n",
    "# !pip install pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect To SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "    import pyodbc \n",
    "    server = r'DESKTOP-77V87AM\\UWA_DW' \n",
    "    database = r'crash_data' \n",
    "    conn = pyodbc.connect(r'Driver={SQL Server};' \n",
    "                        r'Server='+server+\n",
    "                        r';Database='+database+\n",
    "                        r';Trusted_Connection=yes;')\n",
    "    conn.setdecoding(pyodbc.SQL_CHAR, encoding='latin1')\n",
    "    conn.setencoding('latin1')\n",
    "    # cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tables Functions\n",
    "\n",
    "function to create table from dataframe & Output SQL Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "    ## Utility functions \n",
    "    def to_raw(string):\n",
    "        return fr\"{string}\"\n",
    "\n",
    "    def export_sql_query(table_name , query_string):\n",
    "        with open(\"./sql_queries/Create_Table_{0}.sql\".format(table_name), \"w\") as text_file:\n",
    "            print(query_string, file=text_file)\n",
    "\n",
    "    def create_table_query_builder(table_name: str , column_strings: list):\n",
    "        string_array = []\n",
    "        for strings in column_strings:\n",
    "            string_array.append(\"\\t\" + \" \".join(strings) + \",\")\n",
    "        column_string = \"\\n\".join(string_array)\n",
    "        out_string = \"CREATE TABLE {0} (\\n {1}\\n);\".format(table_name , column_string)\n",
    "        return to_raw(out_string)\n",
    "\n",
    "    def create_table(connection , query_string , table_name):\n",
    "        try:\n",
    "            cursor = connection.cursor()  \n",
    "            cursor.execute(query_string)\n",
    "            print(\"Created {0} Tables\".format(table_name))\n",
    "            connection.commit()\n",
    "            ## Write query to file if it succeeds \n",
    "            export_sql_query(table_name , query_string)\n",
    "        except:\n",
    "            print(\"{0} table may already exist\".format(table_name))\n",
    "\n",
    "    def create_table_function(connection , table_name: str , column_strings: list):\n",
    "        query_string = create_table_query_builder(table_name, column_strings)\n",
    "        response = create_table(connection , query_string , table_name)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Data Ingestion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "    def create_sql_insert_string():\n",
    "        print(\"create_sql_insert_string not implemented\")\n",
    "\n",
    "    def export_sql_insert_query(table_name , query_string):\n",
    "        with open(\"./sql_queries/Insert_Data_{0}.sql\".format(table_name), \"w\") as text_file:\n",
    "            print(query_string, file=text_file)\n",
    "\n",
    "    def insert_data_query_builder(table_name: str , df: pd.DataFrame , connection):\n",
    "        df = df.fillna(\"NULL\")\n",
    "        columns = \" , \".join([column for column in df.columns])\n",
    "        out_strings = []\n",
    "        for records in df.to_records(index=False):\n",
    "            values = str(records)\n",
    "            values = str(records).replace(\"'NULL'\" , \"NULL\")\n",
    "            query = \"INSERT INTO {0} ({1}) VALUES {2}\".format(table_name , columns , values )\n",
    "            out_string = insert_data(connection , query , table_name )\n",
    "            if out_string != None:\n",
    "                out_strings.append(out_string)\n",
    "        if len(out_strings) > 0 :\n",
    "            print(\"\\n\".join(out_strings[:5]))\n",
    "\n",
    "    def insert_data(connection , insert_string ,  table_name ):\n",
    "        ## insert values into database \n",
    "        cursor = connection.cursor()\n",
    "        try:\n",
    "            # Insert Dataframe into SQL Server:\n",
    "            cursor.execute(insert_string)\n",
    "            connection.commit()\n",
    "            cursor.close()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            connection.commit()\n",
    "            cursor.close()\n",
    "            return\"Insert Failed: {0} | {1}\".format(table_name , e) \n",
    "        \n",
    "    def insert_dim_values(connection, table_name:str , df: pd.DataFrame , table_data ):\n",
    "        print(\"----------- {0} ----------\".format(table_name))\n",
    "        ## Filter dataframe to only values\n",
    "        df = df[[input[0] for input in table_data if str(input[0]).find(\"FOREIGN KEY\") == -1]]\n",
    "        ## Remove Duplicates \n",
    "        df.drop_duplicates(inplace=True , subset=table_data[0][0])\n",
    "        df.to_csv(\"./csv/{0}.csv\".format(table_name))\n",
    "        insert_data_query_builder(table_name , df ,  connection)\n",
    "\n",
    "    def insert_fact_values(connection, table_name:str , df: pd.DataFrame , table_data ):\n",
    "        print(\"----------- {0} ----------\".format(table_name))\n",
    "        ## Filter dataframe to only values\n",
    "        df = df[[input[0] for input in table_data if str(input[0]).find(\"FOREIGN KEY\") == -1]]\n",
    "        df.to_csv(\"./csv/{0}.csv\".format(table_name))\n",
    "        insert_data_query_builder(table_name , df ,  connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "    ######################### Dimensions #########################\n",
    "    ## Date Dimension\n",
    "    table_data = [\n",
    "        [\"date_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"year\", \"int\", \"NOT NULL\"],\n",
    "        [\"month\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"day\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"quarter\", \"int\", \"NOT NULL\"],\n",
    "        [\"day_of_week\", \"varchar(16)\", \"NOT NULL\"],\n",
    "        # [\"is_holiday\", \"BIT DEFAULT 0\", \"NOT NULL\"], ## Not Implemented\n",
    "    ]\n",
    "    create_table_function(conn, \"DimDate\", table_data)\n",
    "    insert_dim_values(conn,  \"DimDate\", merged_df, table_data)\n",
    "\n",
    "    ## Event description\n",
    "    table_data = [\n",
    "        [\"event_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"event_nature\", \"varchar(50)\"],\n",
    "        [\"event_type\", \"varchar(50)\"],\n",
    "        [\"severity\", \"varchar(50)\"],\n",
    "        [\"longitude\", \"float(24)\"],\n",
    "        [\"latitude\", \"float(24)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimEventDesc\", table_data)\n",
    "    insert_dim_values(conn,  \"DimEventDesc\", merged_df, table_data)\n",
    "\n",
    "    ## Weather Station Dimension\n",
    "    table_data = [\n",
    "        [\"station_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"station_name\", \"varchar(100)\", \"NOT NULL\"],\n",
    "        [\"country\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"region\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"elevation\", \"int\"],\n",
    "        [\"daily_start_date\", \"varchar(50)\"],\n",
    "        [\"daily_end_date\", \"varchar(50)\"],\n",
    "        [\"longitude\", \"float(24)\"],\n",
    "        [\"latitude\", \"float(24)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimWeatherStn\", table_data)\n",
    "    insert_dim_values(conn,  \"DimWeatherStn\", station_dfs, table_data)\n",
    "\n",
    "    ## Road\n",
    "    table_data = [\n",
    "        [\"road_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"road_name\", \"varchar(500)\", \"NOT NULL\"],\n",
    "        [\"common_road_name\", \"varchar(500)\"],\n",
    "        [\"cway\", \"varchar(50)\"],\n",
    "        [\"speed_limit\", \"varchar(100)\"],\n",
    "        [\"network_type\", \"varchar(50)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimRoad\", table_data)\n",
    "    insert_dim_values(conn,  \"DimRoad\", merged_df, table_data)\n",
    "\n",
    "    ## LGA\n",
    "    table_data = [\n",
    "        [\"lga_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"local_gov_name\", \"varchar(100)\", \"NOT NULL\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimLGA\", table_data)\n",
    "    insert_dim_values(conn,  \"DimLGA\", merged_df, table_data)\n",
    "\n",
    "    ## Region\n",
    "    table_data = [\n",
    "        [\"region_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"region_name\", \"varchar(100)\", \"NOT NULL\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimRegion\", table_data)\n",
    "    insert_dim_values(conn,  \"DimRegion\", merged_df, table_data)\n",
    "\n",
    "    ## Location\n",
    "    table_data = [\n",
    "        [\"location_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"road_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"lga_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"region_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"is_intersection\", \"BIT DEFAULT 0\", \"NOT NULL\"],\n",
    "        [\"intersection_no\", \"int\"],\n",
    "        [\"intersection_desc\", \"varchar(500)\"],\n",
    "        [\"FOREIGN KEY (road_id) REFERENCES DimRoad(road_id)\"],\n",
    "        [\"FOREIGN KEY (lga_id) REFERENCES DimLGA(lga_id)\"],\n",
    "        [\"FOREIGN KEY (region_id) REFERENCES DimRegion(region_id)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimLocation\", table_data)\n",
    "    insert_dim_values(conn,  \"DimLocation\", merged_df, table_data)\n",
    "\n",
    "    ## Time Fact\n",
    "    table_data = [\n",
    "        [\"time_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"is_peak_hour\", \"BIT DEFAULT 0\", \"NOT NULL\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"DimTime\", table_data)\n",
    "    insert_dim_values(conn,  \"DimTime\", merged_df, table_data)\n",
    "\n",
    "    ######################### Fact Tables #########################\n",
    "    ## Weather Fact Table\n",
    "    table_data = [\n",
    "        [\"weather_id\", \"varchar(50) primary key\", \"NOT NULL\"],\n",
    "        [\"date_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"station_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"temp_min\", \"float\"],\n",
    "        [\"temp_ave\", \"float\"],\n",
    "        [\"temp_max\", \"float\"],\n",
    "        [\"precipitation_mm\", \"float\"],\n",
    "        [\"wind_direction\", \"float\"],\n",
    "        [\"wind_speed\", \"float\"],\n",
    "        [\"pressure\", \"float\"],\n",
    "        [\"distance\", \"int\"],\n",
    "        [\"FOREIGN KEY (date_id) REFERENCES DimDate(date_id)\"],\n",
    "        [\"FOREIGN KEY (station_id) REFERENCES DimWeatherStn(station_id)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"FactWeather\", table_data)\n",
    "    insert_dim_values(conn,  \"FactWeather\", merged_df, table_data)\n",
    "\n",
    "    ## Crash data facts\n",
    "    table_data = [\n",
    "        [\"crash_id\", \"int primary key\", \"NOT NULL\"],\n",
    "        [\"time_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"date_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"weather_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"location_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"event_id\", \"varchar(50)\", \"NOT NULL\"],\n",
    "        [\"total_bike_involved\", \"int\", \"NOT NULL\"],\n",
    "        [\"total_truck_involved\", \"int\", \"NOT NULL\"],\n",
    "        [\"total_heavy_truck_involved\", \"int\", \"NOT NULL\"],\n",
    "        [\"total_motor_cycle_involved\", \"int\", \"NOT NULL\"],\n",
    "        [\"total_other_vehicles_involved\", \"int\", \"NOT NULL\"],\n",
    "        [\"total_pedestrians_involve\", \"int\", \"NOT NULL\"],\n",
    "        [\"FOREIGN KEY (time_id) REFERENCES DimTime(time_id)\"],\n",
    "        [\"FOREIGN KEY (date_id) REFERENCES DimDate(date_id)\"],\n",
    "        [\"FOREIGN KEY (weather_id) REFERENCES FactWeather(weather_id)\"],\n",
    "        [\"FOREIGN KEY (location_id) REFERENCES DimLocation(location_id)\"],\n",
    "        [\"FOREIGN KEY (event_id) REFERENCES DimEventDesc(event_id)\"],\n",
    "    ]\n",
    "    create_table_function(conn, \"FactCrashData\", table_data)\n",
    "    insert_fact_values(conn,  \"FactCrashData\", merged_df, table_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Tables Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "\tif False:\n",
    "\t# if True:\n",
    "\t\tdrop_tables_str = r\"DROP TABLE FactCrashData, FactWeather ,DimTime,  DimDate,  DimEventDesc, DimWeatherStn, DimLocation, DimRoad, DimLGA  ,DimRegion;\"\n",
    "\n",
    "\t\tif True:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tcursor = conn.cursor()\n",
    "\t\t\t\tcursor.execute(drop_tables_str)\n",
    "\t\t\t\tprint(\"Dropped All Tables\")\n",
    "\t\t\t\tconn.commit()\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(\"Could not drop tables | \" , str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling - Close Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.relational_data:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.commit()\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j CSV Files\n",
    "\n",
    "Create a set of neo4j csv files from source rather than from pervious csv. \n",
    "\n",
    "// maybe also create functions from csv for migrations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if functions_to_run.neo4j:\n",
    "    \n",
    "    ## resets query text files\n",
    "    def reset_queries_file():\n",
    "        with open('./Assignment_2/queries/create_nodes_queries.txt', 'w') as f:\n",
    "            f.write(\"// Create Nodes Functions\\n\\n\")\n",
    "        with open('./Assignment_2/queries/create_relations_queries.txt', 'w') as f:\n",
    "            f.write(\"// Create Relationships Functions \\n\\n\")\n",
    "\n",
    "    ## Appends queries to text files\n",
    "    def append_to_queries_file(query_file_path, query):\n",
    "        with open(query_file_path, 'a') as f:\n",
    "            f.write(query)\n",
    "\n",
    "    ## Creates neo4j_csv_and_queries\n",
    "    def create_node4j_csv(node_name:str , df: pd.DataFrame , table_data: list , relationships: list):\n",
    "        ## Filter dataframe to only values\n",
    "        df = df[[input[0] for input in table_data if str(input[0]).find(\"FOREIGN KEY\") == -1]]\n",
    "        ## Get type values \n",
    "        types = [input[1] for input in table_data if str(input[0]).find(\"FOREIGN KEY\") == -1]\n",
    "        ## Remove Duplicates \n",
    "        df.drop_duplicates(inplace=True , subset=table_data[0][0])\n",
    "        csv_path =\"./Assignment_2/csv/{0}.csv\".format(node_name) \n",
    "        ## Create csv\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        ## Create required Neo4J queries\n",
    "        create_queries(df, types , node_name , relationships)\n",
    "\n",
    "    ## Creates the property string with types \n",
    "    def property_string_creator(prop_name , type_string: str):\n",
    "        if type_string.find(\"int\") != -1:\n",
    "            return \" n.{0} = toInteger(row.{0})\".format(prop_name)\n",
    "        elif type_string.find(\"float\") != -1:\n",
    "            return \" n.{0} = toFloat(row.{0})\".format(prop_name)\n",
    "        elif type_string.find(\"time\") != -1:\n",
    "            return \" n.{0} = Datetime(row.{0})\".format(prop_name)\n",
    "        elif type_string.find(\"date\") != -1:\n",
    "            return \" n.{0} = date(row.{0})\".format(prop_name)\n",
    "        else:\n",
    "            return \" n.{0} = row.{0}\".format(prop_name)\n",
    "\n",
    "    ## Creates a create node query from inputs\n",
    "    def query_create_nodes(node_name:str , node_id:str ):\n",
    "        return \"// Create {0}\\nLOAD CSV WITH HEADERS FROM 'file:///{0}.csv' AS row\\nMERGE (n:{1} {{ {2}: row.{2} }});\\n\\n\\n\".format(\n",
    "            node_name,\n",
    "            node_name,\n",
    "            node_id,\n",
    "        )\n",
    "\n",
    "    ## Creates a create node query from inputs\n",
    "    def query_create_nodes_w_props(node_name:str , node_id:str, node_properties: str ):\n",
    "        return \"// Create {0}\\nLOAD CSV WITH HEADERS FROM 'file:///{0}.csv' AS row\\nMERGE (n:{1} {{ {2}: row.{2} }})\\nON CREATE SET{3};\\n\\n\\n\".format(\n",
    "            node_name,\n",
    "            node_name,\n",
    "            node_id,\n",
    "            node_properties, \n",
    "        )\n",
    "   \n",
    "    ## Creates a relationship query from inputs\n",
    "    def query_create_node_relationships(csv_file_name , nodes , node_ids, edge_name , from_base):\n",
    "        ## Extract key information \n",
    "        base_id = str(node_ids[0])\n",
    "        edge_node_id = str(node_ids[1])\n",
    "        base_node = str(nodes[0])\n",
    "        edge_node = str(nodes[1])\n",
    "        ## format loading string \n",
    "        load_string = \"// Create relationships between {0} and {1}\\nLOAD CSV WITH HEADERS FROM 'file:///{2}.csv' AS row\\n\".format(\n",
    "            base_node,\n",
    "            edge_node, \n",
    "            csv_file_name\n",
    "        )\n",
    "        ## format match strings\n",
    "        match_string = \"MATCH ({0}:{2} {{ {4}: row.{4} }})\\nMATCH ({1}:{3}  {{ {5}: row.{5} }})\\n\".format(\n",
    "            base_node.lower(),\n",
    "            edge_node.lower(),\n",
    "            base_node,\n",
    "            edge_node,\n",
    "            base_id,\n",
    "            edge_node_id, \n",
    "        )\n",
    "        ## format merge string using direction input \n",
    "        merge_string = \"\"\n",
    "        if from_base:\n",
    "            merge_string = \"MERGE ({0})-[:{2}]->({1});\\n\\n\\n\".format(base_node.lower(), edge_node.lower() ,edge_name)\n",
    "        else:\n",
    "            merge_string = \"MERGE ({0})<-[:{2}]-({1});\\n\\n\\n\".format(base_node.lower(), edge_node.lower() ,edge_name)\n",
    "        ## join & return string \n",
    "        query = load_string + match_string + merge_string\n",
    "        return query \n",
    "\n",
    "    ## Create required Neo4J queries\n",
    "    def create_queries(df:pd.DataFrame, types: list,  node_name: str , relationships: list ):\n",
    "        ## establish key variables \n",
    "        node_id = \"\"\n",
    "        prop_strings = []\n",
    "        for index, name in enumerate(df.columns):\n",
    "                if index == 0:\n",
    "                    ## get df index value \n",
    "                    node_id = name\n",
    "                else:\n",
    "                    ## check to ensure field is not an id\n",
    "                    if str(name).find(\"_id\") == -1:\n",
    "                        ## if not get type string & create property string \n",
    "                        type_string = property_string_creator(name, types[index])\n",
    "                        prop_strings.append(type_string)\n",
    "        if len(prop_strings) == 0:\n",
    "            query = query_create_nodes(node_name, node_id)\n",
    "        else:\n",
    "            ## Join property strings \n",
    "            node_properties = \", \".join(prop_strings)\n",
    "            ## generate create query for node \n",
    "            query = query_create_nodes_w_props(node_name, node_id, node_properties)\n",
    "        ## add create query to txt file \n",
    "        append_to_queries_file('./Assignment_2/queries/create_nodes_queries.txt' , query)\n",
    "        ## Loop through relations \n",
    "        for relationship in relationships:\n",
    "            ## Get relationship information \n",
    "            nodes = [node_name , relationship[0]]\n",
    "            node_ids = [node_id , relationship[1]]\n",
    "            edge_name = relationship[2]\n",
    "            from_base = relationship[3]\n",
    "            ## generate relationship query\n",
    "            query = query_create_node_relationships(node_name, nodes, node_ids , edge_name , from_base )\n",
    "            ## add relationship query to text file \n",
    "            append_to_queries_file('./Assignment_2/queries/create_relations_queries.txt' , query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data for speed of queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redced Dataframe to 500 rows\n"
     ]
    }
   ],
   "source": [
    "if functions_to_run.subset_data:\n",
    "    subset_data = merged_df.tail(500)\n",
    "    print(\"Redced Dataframe to {} rows\".format(len(subset_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map data & Generate Queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "if functions_to_run.neo4j:\n",
    "    \n",
    "    ## Reset query files \n",
    "    reset_queries_file()\n",
    "\n",
    "    ######################### Nodes #########################\n",
    "    ## Crash \n",
    "    table_data = [\n",
    "        [\"event_id\", \"string\"],\n",
    "        [\"date_id\", \"string\"],\n",
    "        [\"weather_id\", \"string\"],\n",
    "        [\"lga_id\", \"string\"],\n",
    "        [\"road_id\", \"string\"],\n",
    "        [\"time_id\", \"string\"],\n",
    "        [\"event_nature\", \"string\"],\n",
    "        [\"event_type\", \"string\"],\n",
    "        [\"severity\", \"string\"],\n",
    "        [\"longitude\", \"float\"],\n",
    "        [\"latitude\", \"float\"],\n",
    "        [\"total_bike_involved\", \"int\"],\n",
    "        [\"total_truck_involved\", \"int\"],\n",
    "        [\"total_heavy_truck_involved\", \"int\"],\n",
    "        [\"total_motor_cycle_involved\", \"int\"],\n",
    "        [\"total_other_vehicles_involved\", \"int\"],\n",
    "        [\"total_pedestrians_involve\", \"int\"],\n",
    "    ]\n",
    "    relationships = [\n",
    "        [\"Date\" , \"date_id\", \"ON\" , True], \n",
    "        [\"EventType\" , \"event_type\", \"WAS\" , True], \n",
    "        [\"EventNature\" , \"event_nature\", \"WAS\" , True], \n",
    "        [\"Severity\" , \"severity\", \"WAS\" , True], \n",
    "        [\"LGA\" , \"lga_id\", \"IN\" , True],\n",
    "        [\"Road\" , \"road_id\",\"ON\" , True],\n",
    "        [\"WeatherReading\" , \"weather_id\",\"READING\" , True],\n",
    "    ]\n",
    "    create_node4j_csv( \"Crash\" , subset_data , table_data, relationships)\n",
    "\n",
    "    ## Date \n",
    "    table_data = [\n",
    "        [\"date_id\", \"date\"],\n",
    "        [\"year\", \"int\"],\n",
    "        [\"month\", \"string\"],\n",
    "        [\"day\", \"string\"],\n",
    "        [\"quarter\", \"int\"],\n",
    "        [\"day_of_week\", \"string\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv( \"Date\" , subset_data  , table_data , relationships )\n",
    "\n",
    "    ## Weather Station \n",
    "    table_data = [\n",
    "        [\"station_id\", \"string\"],\n",
    "        [\"station_name\", \"string\"],\n",
    "        [\"country\", \"string\"],\n",
    "        [\"region\", \"string\"],\n",
    "        [\"elevation\", \"int\"],\n",
    "        [\"daily_start_date\", \"string\"],\n",
    "        [\"daily_end_date\", \"string\"],\n",
    "        [\"longitude\", \"float\"],\n",
    "        [\"latitude\", \"float\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv( \"WeatherStn\" , station_dfs , table_data , relationships)\n",
    "\n",
    "    ## Road\n",
    "    table_data = [\n",
    "        [\"road_id\", \"string\"],\n",
    "        [\"road_name\", \"string\"],\n",
    "        [\"common_road_name\", \"string\" ],\n",
    "        [\"cway\", \"string\"],\n",
    "        [\"speed_limit\", \"string\"],\n",
    "        [\"network_type\", \"string\"],\n",
    "    ]\n",
    "    relationships = [] \n",
    "    create_node4j_csv(  \"Road\" , subset_data  , table_data , relationships)\n",
    "\n",
    "    ## LGA\n",
    "    table_data = [\n",
    "        [\"lga_id\", \"string\"],\n",
    "        [\"local_gov_name\", \"string\"],\n",
    "    ]\n",
    "    relationships = [\n",
    "        [\"Region\" , \"region_id\", \"inside\" , True], \n",
    "    ]\n",
    "    create_node4j_csv(  \"LGA\" , subset_data , table_data, relationships )\n",
    "\n",
    "    ## Region\n",
    "    table_data = [\n",
    "        [\"region_id\", \"string\"],\n",
    "        [\"region_name\", \"string\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv(  \"Region\" , subset_data , table_data, relationships )\n",
    "\n",
    "    ## Weather Reading\n",
    "    table_data = [\n",
    "        [\"weather_id\", \"string\"],\n",
    "        [\"date_id\", \"string\"],\n",
    "        [\"station_id\", \"string\"],\n",
    "        [\"temp_min\", \"float\"],\n",
    "        [\"temp_ave\", \"float\"],\n",
    "        [\"temp_max\", \"float\"],\n",
    "        [\"precipitation_mm\", \"float\"],\n",
    "        [\"wind_direction\", \"float\"],\n",
    "        [\"wind_speed\", \"float\"],\n",
    "        [\"pressure\", \"float\"],\n",
    "        [\"distance\", \"int\"],\n",
    "    ]\n",
    "    relationships = [\n",
    "        [\"Date\" , \"date_id\", \"ON\" , True], \n",
    "        [\"WeatherStn\" , \"station_id\", \"FROM\" , True], \n",
    "    ]\n",
    "    create_node4j_csv( \"WeatherReading\" , subset_data , table_data, relationships )\n",
    "\n",
    "    ## EventType \n",
    "    table_data = [\n",
    "        [\"event_type\", \"string\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv( \"EventType\" , subset_data  , table_data , relationships )\n",
    "    \n",
    "    ## EventNature \n",
    "    table_data = [\n",
    "        [\"event_nature\", \"string\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv( \"EventNature\" , subset_data  , table_data , relationships )\n",
    "\n",
    "    ## Severity \n",
    "    table_data = [\n",
    "        [\"severity\", \"string\"],\n",
    "    ]\n",
    "    relationships = []\n",
    "    create_node4j_csv( \"Severity\" , subset_data  , table_data , relationships )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Files to Imports Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash.csv  Deleted\n",
      "Date.csv  Deleted\n",
      "EventNature.csv  Deleted\n",
      "EventType.csv  Deleted\n",
      "LGA.csv  Deleted\n",
      "Location.csv  Deleted\n",
      "Region.csv  Deleted\n",
      "Road.csv  Deleted\n",
      "Severity.csv  Deleted\n",
      "Weather.csv  Deleted\n",
      "WeatherReading.csv  Deleted\n",
      "WeatherStn.csv  Deleted\n",
      "copied Crash.csv\n",
      "copied Date.csv\n",
      "copied EventNature.csv\n",
      "copied EventType.csv\n",
      "copied LGA.csv\n",
      "copied Location.csv\n",
      "copied Region.csv\n",
      "copied Road.csv\n",
      "copied Severity.csv\n",
      "copied Weather.csv\n",
      "copied WeatherReading.csv\n",
      "copied WeatherStn.csv\n"
     ]
    }
   ],
   "source": [
    "if functions_to_run.neo4j:\n",
    "    import os , shutil\n",
    "    imports_location = r\"C:\\Users\\david\\.Neo4jDesktop\\relate-data\\dbmss\\dbms-16f9438d-81f0-4d63-ad67-7d8437cd360c\\import\"\n",
    "    local_csv_location = \"./Assignment_2/csv\"\n",
    "\n",
    "    ## Remove all files in the Neo4j imports folder \n",
    "    def clean_imports_file(directory):   \n",
    "        for filename in os.listdir(directory):\n",
    "            file = os.path.join(directory, filename)\n",
    "            if os.path.isfile(file):\n",
    "                os.remove(file)\n",
    "                print(filename , \" Deleted\")\n",
    "    \n",
    "    ## Copies the created csv files to the Neo4j imports folder \n",
    "    def copy_csv_to_imports_folder(sources_dir, copy_location):\n",
    "        # return \"not implemented\"\n",
    "        for filename in os.listdir(sources_dir):\n",
    "            file = os.path.join(sources_dir, filename)\n",
    "            if os.path.isfile(file):\n",
    "                shutil.copy(file, copy_location)\n",
    "                print('copied', filename)\n",
    "                \n",
    "    clean_imports_file(imports_location)\n",
    "    copy_csv_to_imports_folder(local_csv_location, imports_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cypher Queries \n",
    "\n",
    "For this section I have replaced job terms with their closest reference in the crash dataset. \n",
    "\n",
    "## Write Cypher queries to answer the following questions:\n",
    "- How many crashes are of a given crash type in a specified local government?\n",
    "    >```\n",
    "    >MATCH (n:Crash)-[:in]->(p:LGA {local_gov_name: \"Belmont (C)\"})\n",
    "    >RETURN n.severity AS severity, COUNT(*) AS num_crashes;\n",
    "    >```\n",
    "\n",
    "- Find crashes that share the same crash type.\n",
    "    >```\n",
    "    >MATCH (n:Crash)-[r:WAS]->(p:EventType)\n",
    "    >RETURN n, r,p\n",
    "    >```\n",
    "    Counting number of crash types \n",
    "    >```\n",
    "    >MATCH (n:Crash)\n",
    "    >RETURN n.event_type AS event_type, COUNT(*) AS num_crashes;\n",
    "    >```\n",
    "\n",
    "\n",
    "- Find all local governments that have crashes in different crash types.\n",
    "\n",
    "    >```\n",
    "    >MATCH (lga:LGA)<-[:IN]-(c:Crash)-[:WAS]->(t:EventType)\n",
    "    >RETURN lga.local_gov_name as LGA ,  t.event_type as type, count(*) as Crashes \n",
    "    >ORDER BY Crashes DESC\n",
    "    >```\n",
    "\n",
    "\n",
    "- Find crashes based on the presence of a cyclist. \n",
    "    > ```\n",
    "    > MATCH (n:Crash )\n",
    "    > WHERE n.total_bike_involved > 0\n",
    "    > RETURN n\n",
    "    > ```\n",
    "\n",
    "- Find crashes posted during a specified period of time (2017-2019). \n",
    "Number of crashes for each month between 2017 & 2918\n",
    "\n",
    "\n",
    "   > ```\n",
    "   > MATCH (n:Crash)-[r:ON]->(d:Date)\n",
    "   > WHERE 2016< d.year < 2019\n",
    "   > WITH d.year + \"-\" + d.month as date_value , n\n",
    "   > RETURN distinct(date_value) as Date, count(n) as Crashes ORDER BY Date\n",
    "   > ```\n",
    "\n",
    "\n",
    "   >```\n",
    "   >MATCH (n:Crash)-[r:ON]->(d:Date)\n",
    "   >RETURN distinct(d.month) as Month, count(n) as Crashes ORDER BY Month\n",
    "   >```\n",
    "\n",
    "Write Cypher queries for at least two other meaningful queries that you can think of.\n",
    "\n",
    "- How many crashes of different severity are in each local government region. \n",
    "    >```\n",
    "    >MATCH (n:Crash)-[:in]->(p:LGA {local_gov_name: \"Belmont (C)\"})\n",
    "    >RETURN n.severity AS severity, COUNT(*) AS num_crashes;\n",
    "    >```\n",
    "\n",
    "- How many crashes occurred on rainy days. \n",
    "    >\n",
    "    >```\n",
    "    >MATCH (n:Crash)<-[r:READING]-(w:WeatherReading)\n",
    "    >WHERE w.precipitation_mm > 0\n",
    "    >RETURN count(n) as number_of_crashes\n",
    "    >```\n",
    "    \n",
    "    Return those days & crashes \n",
    "    \n",
    "    >```\n",
    "    >MATCH (n:Crash)<-[r:READING]-(w:WeatherReading)\n",
    "    >WHERE w.precipitation_mm > 0\n",
    "    >RETURN n, r, w\n",
    "    >```\n",
    "\n",
    "- Number of crashes by Severity for each year\n",
    "\n",
    "    >```\n",
    "    >MATCH (n:Crash)-[r:ON]->(d:Date)\n",
    "    >RETURN distinct(d.year) as Date, n.severity as sev, count(n) as Crashes ORDER BY Date\n",
    "    >```\n",
    "\n",
    "- Number of crashes of an event type for each severity \n",
    "    >```\n",
    "    >MATCH (n:Crash)-[:WAS]->(p:EventType {event_type: \"Involving Overtaking\"})\n",
    "    >RETURN n.severity AS severity, COUNT(*) AS num_crashes;\n",
    "    >```\n",
    "\n",
    "\n",
    "- How many pedestrians were involving in crashes by LGA?\n",
    "    >```\n",
    "    >MATCH (n:Crash)-[r:IN]->(l:LGA)\n",
    "    >WHERE n.total_pedestrians_involve > 0\n",
    "    >RETURN l.local_gov_name as local_gov , sum(n.total_pedestrians_involve) as number_of_crashes_involving_pedestrians;\n",
    "    >```\n",
    "\n",
    "- How many Vehicles, bikes, Trucks, & pedestrians were involved in accidents by 10 local governments with the hights total number of crashes \n",
    "\n",
    "    >```\n",
    "    >MATCH (n:Crash)-[:in]->(lga:LGA)\n",
    "    >RETURN lga.local_gov_name as LGA ,  \n",
    "    >    sum(n.total_other_vehicles_involved) as total_other_vehicles , \n",
    "    >    sum(n.total_heavy_truck_involved) as total_heavy_trucks, \n",
    "    >    sum(n.total_bike_involved) as total_bikes, \n",
    "    >    sum(n.total_motor_cycle_involved) as total_motor_cycles,\n",
    "    >    sum(n.total_pedestrians_involve) as total_pedestrians,\n",
    "    >    sum( n.total_other_vehicles_involved + \n",
    "    >         n.total_heavy_truck_involved + \n",
    "    >         n.total_bike_involved + \n",
    "    >         n.total_motor_cycle_involved + \n",
    "    >         n.total_pedestrians_involve) as Total\n",
    "    >ORDER BY Total DESC\n",
    "    >Limit 5\n",
    "    >```\n",
    "\n",
    "\n",
    "- Find the 10 closest accidents to the Perth Weather Stn that occurred on rainy days \n",
    "\n",
    "    >```\n",
    "    >MATCH (stn:WeatherStn {station_name: \"Perth\"})<-[:FROM]-(r:WeatherReading)<->[:READING]-(c:Crash) \n",
    "    >Where r.precipitation_mm > 0\n",
    "    >WITH\n",
    "    >  point({latitude:stn.latitude, longitude:stn.longitude}) AS p1,\n",
    "    >  point({latitude:c.latitude, longitude:c.longitude}) AS p2, c , stn\n",
    "    >RETURN c\n",
    "    >order by toInteger(point.distance(p1, p2)/1000) \n",
    "    >Limit 5\n",
    "    >```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data for Hourly Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ## Create dataframes for weather data \n",
    "    station_dfs = pd.DataFrame()\n",
    "    weather_dfs = pd.DataFrame()\n",
    "    \n",
    "    ## iterate through subset of data\n",
    "    for i , row in crash_gdf.tail(10).iterrows():\n",
    "        x = row.LATITUDE\n",
    "        y = row.LONGITUDE\n",
    "        date_string = row.CRASH_DATE\n",
    "        time_string = row.CRASH_TIME\n",
    "        station_df , weather_df = get_weather_from_crash(x , y, date_string , time_string , daily=False)\n",
    "        station_dfs = pd.concat([station_dfs, station_df], ignore_index=True)\n",
    "        weather_dfs = pd.concat([weather_dfs, weather_df], ignore_index=True)\n",
    "    \n",
    "    station_dfs\n",
    "    weather_dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Single Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    date_string = '15/03/2017'\n",
    "    time_string = '1720'\n",
    "    ## Belmot https://meteostat.net/en/station/94610?t=2022-04-09/2022-04-16\n",
    "    x , y = \"-31.9333, 115.95\".split(\", \")\n",
    "    \n",
    "    station , weather_df = get_weather_from_crash(x , y, date_string , time_string )\n",
    "    \n",
    "    station  \n",
    "    \n",
    "    weather_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('data_warehousing.venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "718518ca707f66d0ab53a5db6c54682284615d5a8988141aae5ae7f66d176474"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
